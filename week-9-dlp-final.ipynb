{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95041,"databundleVersionId":11298874,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.transforms import InterpolationMode\n\n# Train Transform (with vertical flip and increased rotation)\ntrain_transform = transforms.Compose([\n    transforms.Resize([224], interpolation=InterpolationMode.BICUBIC),  # Resize to 384x384\n    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n    transforms.RandomVerticalFlip(p=0.5),  # Random vertical flip with 50% probability\n    transforms.RandomRotation(degrees=30),  # Random rotation within Â±30 degrees\n    transforms.RandomCrop([224], padding=4),  # Random crop with padding\n    transforms.ToTensor(),  # Convert to tensor and scale to [0.0, 1.0]\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])\n\n# Validation (Inference) Transform (unchanged)\nval_transform = transforms.Compose([\n    transforms.Resize([224], interpolation=InterpolationMode.BICUBIC),  # Resize to 384x384\n    transforms.CenterCrop([224]),  # Center crop to 384x384\n    transforms.ToTensor(),  # Convert to tensor and scale to [0.0, 1.0]\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/deep-learning-practice-week-9-image-c-lassifica/\"\ntrain_dataset = datasets.ImageFolder(root_dir+'train',transform=train_transform)\n# full_dataset = datasets.ImageFolder(root_dir+'train',transform=train_transform)\n# train_size  = int(0.9*len(full_dataset))\n# val_size = len(full_dataset) - train_size\n# train_dataset,val_dataset = random_split(\n#     full_dataset,[train_size,val_size]\n# )\n# train_dataset.dataset = datasets.ImageFolder(\n#     root_dir + 'train',transform=train_transform\n# )\n# val_dataset.dataset = datasets.ImageFolder(\n#     root_dir + 'train', transform=val_transform\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\n#     {        \n#     \"train_size\": len(train_dataset),\n#     \"val_size\": len(val_dataset),\n#     # \"test_size\": len(test_dataset)\n#     }\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nnum_cpus = os.cpu_count()\nnum_cpus","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=num_cpus)\n# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=num_cpus)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nmodel = models.vit_h_14(\n    weights = models.ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\nmodel.heads = nn.Sequential(\n    nn.Linear(\n        in_features=1280,\n        out_features = 128,\n        bias=True\n    ),\n    nn.BatchNorm1d(128),\n    nn.GELU(),\n    nn.Dropout(0.25),\n    nn.Linear(\n        in_features=128,\n        out_features=10,\n        bias=True\n    )\n)\nfor param in model.heads.parameters():\n    param.requires_grad = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)  # Wrap the model for multi-GPU support\n\n# Move model to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.NAdam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\n\ndef evaluate_model(model, dataloader, device):\n    \"\"\"\n    Evaluate a PyTorch model on a DataLoader and calculate accuracy and F1 score.\n\n    Args:\n        model (torch.nn.Module): Trained PyTorch model.\n        dataloader (torch.utils.data.DataLoader): DataLoader for evaluation.\n        device (torch.device): Device to run the evaluation on ('cuda' or 'cpu').\n\n    Returns:\n        accuracy (float): Accuracy of the model on the dataset.\n        f1 (float): F1 score of the model on the dataset (macro-averaged).\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for inputs, labels in tqdm(dataloader, desc=\"Validating Model\", total=len(dataloader)):\n            # Move inputs and labels to the specified device\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Get predicted class indices\n            preds = torch.argmax(outputs, dim=1)\n            \n            # Accumulate predictions and labels\n            all_preds.append(preds.cpu())\n            all_labels.append(labels.cpu())\n\n    # Concatenate all predictions and labels into single tensors\n    all_preds = torch.cat(all_preds)\n    all_labels = torch.cat(all_labels)\n\n    # Calculate accuracy\n    correct = (all_preds == all_labels).sum().item()\n    total = all_labels.size(0)\n    accuracy = correct / total\n\n    # Calculate F1 score (macro-averaged)\n    f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average=\"macro\")\n\n    return accuracy, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\nimport os\n\nnum_epochs = 5\n\n# Initialize GradScaler for mixed precision training\nscaler = GradScaler()\n\n# Empty unused variables and clear GPU memory\ntorch.cuda.empty_cache()\n\n# Directory to save the latest checkpoint\ncheckpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    train_loss = 0\n    with tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", unit=\"batch\") as pbar:\n        for images, labels in pbar:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass with mixed precision\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            # Backward pass with scaled gradients\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_loss += loss.item()\n            pbar.set_postfix({\"loss\": f\"{train_loss/len(train_dataloader):.4f}\"})\n    \n    # Evaluate the model after each epoch\n    # accuracy, f1 = evaluate_model(model, val_dataloader, device)\n    # print(f\"Epoch {epoch+1} - accuracy: {accuracy}, f1_score: {f1}\")\n\n    # Save the latest checkpoint\n    # checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n    # torch.save({\n    #     'epoch': epoch + 1,\n    #     'model_state_dict': model.state_dict(),\n    #     'optimizer_state_dict': optimizer.state_dict(),\n    #     'scaler_state_dict': scaler.state_dict(),\n    #     'loss': train_loss / len(train_dataloader),\n    #     'accuracy': accuracy,\n    #     'f1_score': f1,\n    # }, checkpoint_path)\n    # print(f\"Checkpoint saved at {checkpoint_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.io import read_image\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os\nimport torch\n\ndef classify_images_to_csv(image_folder, model, transform, output_csv):\n    \"\"\"\n    Classifies images in a folder using a PyTorch model and saves predictions to a CSV file,\n    with progress tracking using tqdm.\n\n    Args:\n        image_folder (str): Path to the folder containing images.\n        model (torch.nn.Module): Trained PyTorch model for classification.\n        transform (torchvision.transforms.Compose): Transformations for preprocessing images.\n        output_csv (str): Path for saving the output CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Prepare a list to store results\n    results = []\n\n    # List all image files\n    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    total = len(image_files)\n\n    # Process each image in the folder with tqdm progress bar\n    for image_name in tqdm(image_files, desc=\"Processing Images\", total=total):\n        # Read and preprocess the image\n        image_path = os.path.join(image_folder, image_name)\n        image = Image.open(image_path).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)  # Add batch dimension\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(image)  # Get raw outputs\n            probabilities = torch.softmax(outputs, dim=1)  # Apply softmax\n            label = torch.argmax(probabilities, dim=1).item()  # Get the predicted label\n\n        # Store the result\n        results.append({\n            \"Image_ID\": image_name.split('.')[0],\n            \"Label\": label\n        })\n\n    # Save results to a CSV file\n    df = pd.DataFrame(results)\n    df.to_csv(output_csv, index=False)\n    print(f\"Predictions saved to {output_csv}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classify_images_to_csv('/kaggle/input/deep-learning-practice-week-9-image-c-lassifica/test',model,val_transform,\"/kaggle/working/submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}